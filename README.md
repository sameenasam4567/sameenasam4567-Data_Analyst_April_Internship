# sameenasam4567-Data_Analytics_April_Internship


Task 1
       First, we will gather data from a public website. Specifically, I am extracting information from YellowPages. The focus of my project is dentists in the city of Atlanta. We can achieve this by employing web scraping techniques, or alternatively, by utilizing instant web scraping extensions to extract data from public websites. When utilizing web scraping, you have the flexibility to select data according to your preferences. On the other hand, instant web scraping automatically selects data for you. After gathering the data, we will proceed to clean it using appropriate tools.

       Web scraping involves extracting data from websites by parsing the HTML code of web pages. This technique allows us to gather specific information, such as contact details, prices, or product descriptions, from various websites for analysis or other purposes. Web scraping is commonly used in fields like research, data analysis, market intelligence, and competitor analysis.

Instant scraping data extensions are browser extensions or plugins designed to simplify the process of data extraction from web pages. Unlike traditional web scraping, where users may need to write custom scripts or use specialized software, instant scraping extensions typically offer a user-friendly interface that allows users to select specific data elements on a webpage and extract them with a single click or action. These extensions are particularly useful for individuals who may not have coding skills but still need to gather data from websites efficiently. They can be employed for tasks such as extracting contact information, product details, or pricing from multiple web pages quickly and effortlessly.


Task 2


       Data Cleaning



       Data cleaning tools are software applications or libraries designed to identify and rectify errors, inconsistencies, and inaccuracies in datasets. These tools are essential for ensuring data quality and reliability before analysis or further processing. Some common data cleaning tools include:

1. OpenRefine: This is a powerful open-source tool for cleaning and transforming messy data. It provides functionalities for exploring, cleaning, and transforming large datasets with ease.

2. Trifacta Wrangler: Trifacta Wrangler is a data preparation platform that offers intuitive visualizations and suggestions to guide users through the cleaning process. It automates many aspects of data cleaning, making it suitable for users with varying levels of technical expertise.

3. Pandas (Python Library): Pandas is a popular Python library for data manipulation and analysis. It offers robust data cleaning capabilities, including methods for handling missing values, removing duplicates, and transforming data.

4. DataWrangler (by Google): DataWrangler is a web-based tool developed by Google for data cleaning and transformation tasks. It provides an interactive interface for visually exploring and cleaning data.

5. OpenRefine (formerly Google Refine): OpenRefine is an open-source tool for working with messy data: cleaning it; transforming it from one format into another; and extending it with web services and external data.

6. Dedupe.io: Dedupe.io is a data cleaning tool specifically designed for identifying and removing duplicate records from large datasets. It uses advanced algorithms to identify similar records and merge them intelligently.

7. **Excel**: While not specialized for data cleaning, Excel offers basic functionalities such as sorting, filtering, and conditional formatting, which can be used for simple data cleaning tasks.

   So here i have used Excel for data Cleaning
